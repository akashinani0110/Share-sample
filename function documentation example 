# Import required libraries
import json  # For reading JSON configuration files
import pandas as pd  # For reading and processing Excel data
import numpy as np  # For numerical operations like replacing NaN
import sqlite3  # For connecting and working with SQLite databases
import datetime  # For timestamping updates
import streamlit as st  # For displaying updates in a Streamlit app

def map_config(json_config_path: str = "config/config.json",
               update_config_path: str = "config/update_config.json",
               data_files: list = None,
               cma_db_path=None):
    """
    Fetch data from Excel files using configuration mappings and update a database.

    Parameters:
        json_config_path (str): Path to the main configuration JSON file.
        update_config_path (str): Path to the update status configuration file.
        data_files (list): Specific file identifiers to process. If None, defaults will be used.
        cma_db_path (str): Path to the SQLite database to update.
    """

    # Load configuration JSON files
    try:
        update_config = json.load(open(update_config_path))  # Load update tracking config
        config = json.load(open(json_config_path))  # Load main session configuration
    except FileNotFoundError:
        raise ValueError(f"Error: Config file {json_config_path} not found.")  # Error if file doesn't exist
    except json.JSONDecodeError:
        raise ValueError(f"Error: Invalid format in config file {json_config_path}.")  # Error if JSON is malformed

    # Establish SQLite connection
    conn = sqlite3.connect(cma_db_path)  # Connect to the provided SQLite database
    cursor = conn.cursor()  # Create a cursor for executing SQL commands

    # Check if a "Log" table already exists in the database
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='Log'")
    log_table_exists = cursor.fetchone()  # Returns None if log table doesn't exist

    # If no data files are specified, use a predefined list
    if data_files is None:
        data_files = [
            "metadata_file", "CMA_proposed_data", "CMA_current_data",
            "cma_aftertax_value_proposed", "cma_aftertax_value_current",
            "cma_output_proposed", "cma_output_current", "cma_output_transition",
            "cma_propossed_output_visualization", "cma_factors_output_proposed",
            "cma_factors_output_current", "cma_factors_output_transition"
        ]

    # Process each data file listed
    for files in data_files:
        st.markdown(f"<b>Started Updating {files}</b>", unsafe_allow_html=True)  # Display start message

        # Get the folder and file path from config
        folder_mapping = config["session_data_files"][files]["path"]  # Logical folder key
        folder = config["directory_path"].get(folder_mapping)  # Actual folder path
        file_name = config["session_data_files"][files]['file']  # Excel file name
        file_path = f"{folder}/{file_name}"  # Full file path

        try:
            # Load log sheet from Excel
            log_data = pd.read_excel(file_path, sheet_name="Log", header=None)  # Read raw log data

            # Determine which sheets to log
            sheets_to_filter = list(config["session_data_files"][files]['sheets'].keys())  # Sheet keys
            sheets_to_filter.insert(0, "Timestamp")  # Add timestamp to filter list

            # Filter log data rows based on sheet names and standardize
            log_data = log_data[log_data[0].isin(sheets_to_filter)]
            log_data[0] = log_data[0].replace("Timestamp", file_name)  # Replace Timestamp with filename

            # Assign proper column names and metadata
            log_data.columns = ["Sheet_name", "Timestamp"]
            log_data["Updated_on"] = pd.Timestamp.now()  # Current timestamp
            log_data["folder_path"] = folder  # File folder
            log_data["file_name"] = file_name  # File name

            # Write log data to database
            if log_table_exists:
                log_data.to_sql("Log", conn, if_exists="append", index=False)
            else:
                log_data.to_sql("Log", conn, if_exists="replace", index=False)

            # Show success message
            st.markdown(
                f"<div style='color:blue; font-size:14px'>Log file updated successfully</div>",
                unsafe_allow_html=True
            )

        except Exception as e:
            # Show error message in Streamlit
            st.markdown(
                f"<div style='color:red'>An error occurred while reading {file_name}: {e}</div>",
                unsafe_allow_html=True
            )
            continue  # Skip this file and move to the next

        excel_file_path = file_path  # Set Excel file path

        # Loop through each sheet configured for the current file
        for sheet_name, sheet_info in config["session_data_files"][files]["sheets"].items():
            # Remove 'repeat' keyword if present
            sheet = sheet_name.replace("repeat", "") if "repeat" in sheet_name else sheet_name

            # Define number of rows to skip (e.g., header info)
            skip = sheet_info.get("skiprows", 0)

            # Read the sheet into a DataFrame
            df = pd.read_excel(excel_file_path, sheet_name=sheet, skiprows=skip)

            # Convert any configured date columns
            if "date_column" in sheet_info:
                for column in sheet_info["date_column"]:
                    df[column] = pd.to_datetime(df[column]).dt.strftime("%Y-%m-%d")

            # Apply filtering if specified
            if 'filter' in sheet_info:
                for column, value in sheet_info['filter'].items():
                    if isinstance(value, dict):  # Dynamic filter via DB query
                        data = generate_query(table_name=value["from"], conn=conn)
                        filter_values = data[value["on"]].tolist()
                        if value.get("make_space_to_underscore"):
                            filter_values = [i.replace(" ", "_") for i in filter_values]
                    else:
                        filter_values = value
                    df = df[df[column].isin(filter_values)]  # Apply filter

            # Set index column if specified
            if "set_index" in sheet_info:
                df.set_index(sheet_info["set_index"], inplace=True)

            filtered_df = df  # Assign filtered data

            # Keep only specified columns if defined
            if "column_name" in sheet_info:
                columns = sheet_info['column_name']
                filtered_df = filtered_df[columns]

            # Handle joins if defined
            if "join" in sheet_info:
                join_data = sheet_info.get("join")
                join_skip = join_data.get("skiprows", 8)
                df_join = pd.read_excel(
                    excel_file_path,
                    sheet_name=join_data.get("sheet_name"),
                    skiprows=join_skip
                )
                if "column_name" in join_data:
                    df_join = df_join[join_data.get("column_name")]
                filtered_df = pd.merge(filtered_df, df_join, on=join_data.get("columns"))

            # Replace NaNs with specified values
            if "modify_na" in sheet_info:
                filtered_df = filtered_df.replace(np.nan, sheet_info["modify_na"], regex=True)

            # Calculate summary statistics if requested
            if sheet_info.get("calculate_timeseries_stats"):
                df_stats = filtered_df.iloc[:, 1:]  # Exclude index
                filtered_df = pd.DataFrame({
                    'min': df_stats.min(),
                    'max': df_stats.max(),
                    'range_05': df_stats.quantile(0.05),
                    'range_25': df_stats.quantile(0.25),
                    'range_45': df_stats.quantile(0.45),
                    'median': df_stats.median(),
                    'range_55': df_stats.quantile(0.55),
                    'range_75': df_stats.quantile(0.75),
                    'range_95': df_stats.quantile(0.95),
                    'last_point': df_stats.iloc[-1]
                })

            # Store final processed data into SQL table
            filtered_df.to_sql(sheet_info["map_to"], conn, if_exists="replace", index=True)

            # Show success message
            st.markdown(
                f"<div style='color:green; font-size:14px'>{sheet_info['map_to']} updated successfully</div>",
                unsafe_allow_html=True
            )

    # Show the log file in the app
    st.write("CMA Log File")
    log_file = generate_query(table_name="Log", conn=conn)
    st.write(log_file)

    # Calculate and show the latest update timestamp
    cma_last_updated = max(pd.to_datetime(log_file["Timestamp"]))
    st.write(cma_last_updated)

    try:
        # Update the update config file to mark update as complete
        update_config["update_config"] = False
        with open(update_config_path, 'w') as f:
            json.dump(update_config, f, indent=4)
            print(f"Config file updated: {update_config_path}")

        # Create a table for updated timestamps
        last_updated = datetime.datetime.now().date().strftime("%d %b %Y")
        cma_output_updated = pd.to_datetime(cma_last_updated).date().strftime('%Y')

        updated_dates_table = pd.DataFrame({
            "updated_field": ["last_updated", "cma_output_updated"],
            "updated_on": [last_updated, cma_output_updated]
        })

        updated_dates_table.to_sql(update_config["updated_on_table_name"], conn, if_exists="replace", index=True)
    except Exception as e:
        # Log error during update
        print(f"Error updating config file {update_config_path}: {e}")

    return None  # Explicit return for clarity




